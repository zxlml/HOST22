{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重复100次实验  取平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T20:44:08.271143Z",
     "start_time": "2019-02-11T20:44:07.434388Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE                # 过抽样处理库SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler  # 欠抽样处理库RandomUnderSample\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA           #加载PCA算法包\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['data.xlsx','AES_T400.xlsx','AES_T700.xlsx','AES_T800.xlsx','AES_T900.xlsx',\n",
    "                  'AES_T1200.xlsx','AES_T1300.xlsx','AES_T1400.xlsx','AES_T2000.xlsx',]\n",
    "\n",
    "def create_numerics(data):\n",
    "    # Get nominal columns\n",
    "    nominal_cols = data.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Turn nominal to numeric\n",
    "    for nom in nominal_cols:\n",
    "        enc = LabelEncoder()\n",
    "        enc.fit(data[nom])\n",
    "        data[nom] = enc.transform(data[nom])\n",
    "    return data\n",
    "    \n",
    "def prepare_data(case = 0 , feature_choice = 0, data_id = 0, ispca = 0):\n",
    "\n",
    "    data = pd.read_excel(data_names[data_id])\n",
    "    data = data.dropna()\n",
    "\n",
    "    data = create_numerics(data)\n",
    "    data = shuffle(data)\n",
    "    \n",
    "    data = data.values\n",
    "    y = data[:,-1]\n",
    "    x = data[:,0:-1]\n",
    "    if(ispca == 1):\n",
    "        pca = PCA(n_components=0.9)\n",
    "        pca.fit(x)\n",
    "        x = pca.transform(x)\n",
    "    \n",
    "    if(feature_choice == 1):    #我们的  先验知识\n",
    "        l=[0,4,8,12]\n",
    "        x = data[:,l]\n",
    "    elif(feature_choice == 2):  # 非均衡下 SHAP的结果  和   任何情况下 PI 的结果\n",
    "        l=[0,1,3,4,8,12]  \n",
    "        x = data[:,l]    \n",
    "    elif(feature_choice == 3):  # 均衡下  SHAP 的结果  \n",
    "        l=[0,4,8,10,12,15]\n",
    "        x = data[:,l]    \n",
    "    if(data_id == 0):    \n",
    "        print(\"Original samples:\",len(y))\n",
    "        if(case ==1):\n",
    "            model_smote = SMOTE(random_state=42)                      # 建立SMOTE模型对象\n",
    "            x, y = model_smote.fit_sample(x, y)         # 输入数据做过抽样处理\n",
    "        elif(case ==2):\n",
    "            model_RandomUnderSample = RandomUnderSampler(random_state=42)   # 建立RandomUnderSampler模型对象\n",
    "            x, y = model_RandomUnderSample.fit_sample(x, y)   # 输入数据做欠抽样处理\n",
    "        elif(case ==3):\n",
    "            oversample = BorderlineSMOTE(random_state=42)\n",
    "            x, y = oversample.fit_resample(x, y)\n",
    "        elif(case ==4):\n",
    "            smt = SMOTETomek(random_state=42)\n",
    "            x,y =  smt.fit_sample(x, y)\n",
    "        elif(case ==5):\n",
    "            smt = SMOTEENN(random_state=42)\n",
    "            x,y =  smt.fit_sample(x, y)    \n",
    "        print(\"New samples:\",len(y))\n",
    "        print(\"Tro samples:\",y.sum())\n",
    "        \n",
    "    normal = len(y) - y.sum()\n",
    "    trojan = y.sum()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x = scaler.fit_transform(x)\n",
    "    return(x,y,normal,trojan)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=ratio, random_state=1)\n",
    "#     return(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重复实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T20:44:08.306098Z",
     "start_time": "2019-02-11T20:44:08.297298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 6915\n",
      "New samples: 11898\n",
      "Tro samples: 5914\n",
      "Original samples: 6915\n",
      "New samples: 11915\n",
      "Tro samples: 5931\n",
      "Original samples: 6915\n",
      "New samples: 11930\n",
      "Tro samples: 5946\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11910\n",
      "Tro samples: 5926\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11927\n",
      "Tro samples: 5943\n",
      "Original samples: 6915\n",
      "New samples: 11928\n",
      "Tro samples: 5944\n",
      "Original samples: 6915\n",
      "New samples: 11928\n",
      "Tro samples: 5944\n",
      "Original samples: 6915\n",
      "New samples: 11927\n",
      "Tro samples: 5943\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11909\n",
      "Tro samples: 5925\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11915\n",
      "Tro samples: 5931\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11925\n",
      "Tro samples: 5941\n",
      "Original samples: 6915\n",
      "New samples: 11905\n",
      "Tro samples: 5921\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11909\n",
      "Tro samples: 5925\n",
      "Original samples: 6915\n",
      "New samples: 11909\n",
      "Tro samples: 5925\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11901\n",
      "Tro samples: 5917\n",
      "Original samples: 6915\n",
      "New samples: 11914\n",
      "Tro samples: 5930\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11925\n",
      "Tro samples: 5941\n",
      "Original samples: 6915\n",
      "New samples: 11920\n",
      "Tro samples: 5936\n",
      "Original samples: 6915\n",
      "New samples: 5933\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11926\n",
      "Tro samples: 5942\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11920\n",
      "Tro samples: 5936\n",
      "Original samples: 6915\n",
      "New samples: 11920\n",
      "Tro samples: 5936\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11902\n",
      "Tro samples: 5918\n",
      "Original samples: 6915\n",
      "New samples: 11928\n",
      "Tro samples: 5944\n",
      "Original samples: 6915\n",
      "New samples: 11923\n",
      "Tro samples: 5939\n",
      "Original samples: 6915\n",
      "New samples: 11915\n",
      "Tro samples: 5931\n",
      "Original samples: 6915\n",
      "New samples: 11912\n",
      "Tro samples: 5928\n",
      "Original samples: 6915\n",
      "New samples: 11911\n",
      "Tro samples: 5927\n",
      "Original samples: 6915\n",
      "New samples: 11913\n",
      "Tro samples: 5929\n",
      "Original samples: 6915\n",
      "New samples: 11911\n",
      "Tro samples: 5927\n",
      "Original samples: 6915\n",
      "New samples: 11923\n",
      "Tro samples: 5939\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11923\n",
      "Tro samples: 5939\n",
      "Original samples: 6915\n",
      "New samples: 11913\n",
      "Tro samples: 5929\n",
      "Original samples: 6915\n",
      "New samples: 11915\n",
      "Tro samples: 5931\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11926\n",
      "Tro samples: 5942\n",
      "Original samples: 6915\n",
      "New samples: 11907\n",
      "Tro samples: 5923\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11909\n",
      "Tro samples: 5925\n",
      "Original samples: 6915\n",
      "New samples: 11926\n",
      "Tro samples: 5942\n",
      "Original samples: 6915\n",
      "New samples: 11904\n",
      "Tro samples: 5920\n",
      "Original samples: 6915\n",
      "New samples: 11912\n",
      "Tro samples: 5928\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11910\n",
      "Tro samples: 5926\n",
      "Original samples: 6915\n",
      "New samples: 11926\n",
      "Tro samples: 5942\n",
      "Original samples: 6915\n",
      "New samples: 11920\n",
      "Tro samples: 5936\n",
      "Original samples: 6915\n",
      "New samples: 11925\n",
      "Tro samples: 5941\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11921\n",
      "Tro samples: 5937\n",
      "Original samples: 6915\n",
      "New samples: 11901\n",
      "Tro samples: 5917\n",
      "Original samples: 6915\n",
      "New samples: 11925\n",
      "Tro samples: 5941\n",
      "Original samples: 6915\n",
      "New samples: 11909\n",
      "Tro samples: 5925\n",
      "Original samples: 6915\n",
      "New samples: 11925\n",
      "Tro samples: 5941\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11929\n",
      "Tro samples: 5945\n",
      "Original samples: 6915\n",
      "New samples: 11913\n",
      "Tro samples: 5929\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11926\n",
      "Tro samples: 5942\n",
      "Original samples: 6915\n",
      "New samples: 11920\n",
      "Tro samples: 5936\n",
      "Original samples: 6915\n",
      "New samples: 11906\n",
      "Tro samples: 5922\n",
      "Original samples: 6915\n",
      "New samples: 11924\n",
      "Tro samples: 5940\n",
      "Original samples: 6915\n",
      "New samples: 11922\n",
      "Tro samples: 5938\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11936\n",
      "Tro samples: 5952\n",
      "Original samples: 6915\n",
      "New samples: 11911\n",
      "Tro samples: 5927\n",
      "Original samples: 6915\n",
      "New samples: 11931\n",
      "Tro samples: 5947\n",
      "Original samples: 6915\n",
      "New samples: 11916\n",
      "Tro samples: 5932\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11922\n",
      "Tro samples: 5938\n",
      "Original samples: 6915\n",
      "New samples: 11905\n",
      "Tro samples: 5921\n",
      "Original samples: 6915\n",
      "New samples: 11924\n",
      "Tro samples: 5940\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11910\n",
      "Tro samples: 5926\n",
      "Original samples: 6915\n",
      "New samples: 11918\n",
      "Tro samples: 5934\n",
      "Original samples: 6915\n",
      "New samples: 11923\n",
      "Tro samples: 5939\n",
      "Original samples: 6915\n",
      "New samples: 11917\n",
      "Tro samples: 5933\n",
      "Original samples: 6915\n",
      "New samples: 11922\n",
      "Tro samples: 5938\n",
      "Original samples: 6915\n",
      "New samples: 11919\n",
      "Tro samples: 5935\n",
      "Original samples: 6915\n",
      "New samples: 11911\n",
      "Tro samples: 5927\n"
     ]
    }
   ],
   "source": [
    "# sampling cases: 0不处理    1 过采样Smote  2 随机欠采样      3 Borderline Smote  4 Smote+Toek 5 SmoteEEN\n",
    "\n",
    "# 特征选择      ：0： 全选   1： 1 4 8 12   2：1 2 4 5 9 13   3：1 4 5 9 13 16\n",
    "\n",
    "# 数据选择 data_names = ['data.xlsx','AES_T400.xlsx','AES_T700.xlsx','AES_T800.xlsx','AES_T900.xlsx',\n",
    "# 'AES_T1200.xlsx','AES_T1300.xlsx','AES_T1400.xlsx','AES_T2000.xlsx',]\n",
    "# 0 是训练集  其他 1-8 是测试集\n",
    "\n",
    "# PCA处理       ：0 则不PCA  1  则PCA\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sampling = 5     # 过采样技术  建议 5\n",
    "    id_features = 1  # 降维技术    建议 2\n",
    "    is_pca = 0       # 是否PCA     建议 0 \n",
    "    normal = []\n",
    "    trojan = []\n",
    "    for i in range(0,100):\n",
    "        x_train, y_train,n,t = prepare_data(case = sampling , feature_choice = id_features, data_id = 0, ispca = is_pca)\n",
    "        normal.append(n)\n",
    "        trojan.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均正常数据规模 5924.16\n",
      "平均木马数据规模 5933.62\n"
     ]
    }
   ],
   "source": [
    "    print(\"平均正常数据规模\",np.mean(normal))\n",
    "    print(\"平均木马数据规模\",np.mean(trojan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffd07b64928e1e927f34974c45036581f12be0ff4d37550e26c4888777a51c9b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
